p8105_hw3_smy2122
================
Sarah Younes

First, I will load all the packages I will need for this homework
assignment: tidyverse.

``` r
library(tidyverse)
library(p8105.datasets)
```

# Problem 1

First, I will load the Instacart data set.

``` r
data("instacart")

instacart_df =
  instacart |>
  as_tibble()
```

The Instacart data set contains 15 variables and 1384617 observations.
There are 39123 products found in 131209 orders from 131209 distinct
users.

Furthermore, there are 134 aisles. The aisle most frequently ordered
from is aisle 83, otherwise known as fresh vegetables. According to the
code below, the fresh vegetables and fresh fruits aisles had the most
products.

``` r
instacart_df |>
  count(aisle) |>
  arrange(desc(n))
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ℹ 124 more rows

Below is a plot that shows the number of items ordered in each aisle for
aisles with more than 10,000 items ordered.

``` r
instacart_df |>
  count(aisle) |>
  filter(n > 10000) |>
  mutate(aisle = fct_reorder(aisle, n)) |>
  ggplot(aes(x = aisle, y = n, color = aisle)) +
  geom_point() +
  labs(title = "Number of items ordered in each aisle") +
  labs(caption = "Among aisles with more than 10,000 items ordered")
```

![](p8105_hw3_smy2122_files/figure-gfm/instacart%20plot-1.png)<!-- -->

Now, I will make a table showing the top 3 most popular items in 3
aisles (baking ingredients, dog food care, and packaged vegetables
fruits) and the number of times each item was ordered.

``` r
instacart_df |>
  filter(
    aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")
  ) |>
  select(aisle, product_name, order_id) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()
```

| aisle                      | product_name                                    |    n | rank |
|:---------------------------|:------------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                            | 3324 |    1 |
| packaged vegetables fruits | Organic Raspberries                             | 1920 |    2 |
| packaged vegetables fruits | Organic Blueberries                             | 1692 |    3 |
| baking ingredients         | Light Brown Sugar                               |  157 |    1 |
| baking ingredients         | Pure Baking Soda                                |  140 |    2 |
| baking ingredients         | Organic Vanilla Extract                         |  122 |    3 |
| dog food care              | Organix Grain Free Chicken & Vegetable Dog Food |   14 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe             |   13 |    2 |
| dog food care              | Original Dry Dog                                |    9 |    3 |

Now, I will make a table showing the mean hour of the day at which two
items were ordered on each day of the week.

``` r
instacart_df |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable()
```

| product_name     |        0 |        1 |        2 |        3 |        4 |        5 |        6 |
|:-----------------|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 | 15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 | 14.25000 | 11.55172 | 12.78431 | 11.93750 |

# Problem 2

First, I will load the BRFSS dataset.

``` r
library(p8105.datasets)
data("brfss_smart2010")
```

Now, I will create a data frame for the data set and do some data
cleaning: formatting the data to use appropriate variable names,
filtering the data set to focus on the “Overall Health” topic, and
organizing responses as a factor taking levels ordered from “Poor” to
“Excellent.”

``` r
brffs_df =
  p8105.datasets::brfss_smart2010 |>
  janitor::clean_names() |>
  separate(locationdesc, into = c("state", "county"), sep = 5) |>
  select(everything(), -state) |>
  rename(state = locationabbr) |>
  filter(
    topic == "Overall Health",
    response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) |>
  mutate(response = fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) |>
  select(year, state, county, topic, response, data_value)
```

Now, I will identify the states that were observed at 7 or more
locations in 2002 and 2010.

``` r
brffs_df |>
  select(year, state, county) |>
  filter(year == 2002) |>
  distinct() |>
  group_by(state, year) |>
  summarize(n = n()) |>
  filter(n >= 7) |>
  arrange(year, desc(n)) |>
  knitr::kable()
```

| state | year |   n |
|:------|-----:|----:|
| PA    | 2002 |  10 |
| MA    | 2002 |   8 |
| NJ    | 2002 |   8 |
| CT    | 2002 |   7 |
| FL    | 2002 |   7 |
| NC    | 2002 |   7 |

``` r
brffs_df |>
  select(year, state, county) |>
  filter(year == 2010) |>
  distinct() |>
  group_by(state, year) |>
  summarize(n = n()) |>
  filter(n >= 7) |>
  arrange(year, desc(n)) |>
  knitr::kable()
```

| state | year |   n |
|:------|-----:|----:|
| FL    | 2010 |  41 |
| NJ    | 2010 |  19 |
| TX    | 2010 |  16 |
| CA    | 2010 |  12 |
| MD    | 2010 |  12 |
| NC    | 2010 |  12 |
| NE    | 2010 |  10 |
| WA    | 2010 |  10 |
| MA    | 2010 |   9 |
| NY    | 2010 |   9 |
| OH    | 2010 |   8 |
| CO    | 2010 |   7 |
| PA    | 2010 |   7 |
| SC    | 2010 |   7 |

Now, I will make a spaghetti plot showing the average value over time
for each state from 2002 to 2010.

``` r
brffs_df |>
  filter(response == "Excellent") |>
  group_by(year, state) |>
  summarize(mean_data_value = mean(data_value)) |>
  ggplot(aes(x = year, y = mean_data_value, group = state, color = state)) +
  geom_line() +
  labs(
    x = "Year",
    y = "Mean value",
    color = "State",
    title = "The average data value over time for each state from 2002 to 2010")
```

![](p8105_hw3_smy2122_files/figure-gfm/spaghetti%20plot-1.png)<!-- -->

Below is a two-panel plot showing the distribution of data value for
responses among locations in New York State in 2006 and 2010.

``` r
brffs_df |>
  filter(
    year == c(2006, 2010),
    state == "NY") |>
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
  labs(
    x = "Response",
    y = "Data value",
    color = "Location",
    title = "Distribution of data values for responses among locations in New York State in 2006 and 2010"
  )
```

    ## Warning: There was 1 warning in `filter()`.
    ## ℹ In argument: `year == c(2006, 2010)`.
    ## Caused by warning in `year == c(2006, 2010)`:
    ## ! longer object length is not a multiple of shorter object length

![](p8105_hw3_smy2122_files/figure-gfm/two-panel%20plot-1.png)<!-- -->

# Problem 3

First, I will import, clean, and tidy the two data sets.

``` r
demographic_df =
  read_csv(
    "./data/nhanes_covar.csv",
    skip = 4,
    na = "NA") |>
  janitor::clean_names() |>
  filter(age >= 21) |>
  drop_na() |>
  mutate(
    sex =
      case_match(
        sex,
        1 ~ "male",
        2 ~ "female"),
        sex = as.factor(sex)
  ) |>
  mutate(
    education =
      case_match(
        education,
        1 ~ "less than high school",
        2 ~ "high school equivalent",
        3 ~ "more than high school"),
    education = as.factor(education)
  )

accelerometer_df =
  read_csv("./data/nhanes_accel.csv") |>
  janitor::clean_names()
```

Now, I will merge the two data sets.

``` r
merged_df =
  inner_join(demographic_df, accelerometer_df)
```

    ## Joining with `by = join_by(seqn)`

Below is a table of the number of men and women in each education
category.

``` r
demographic_df |>
  count(sex, education) |>
  pivot_wider(
    names_from = "education",
    values_from = n
  ) |>
  knitr::kable()
```

| sex    | high school equivalent | less than high school | more than high school |
|:-------|-----------------------:|----------------------:|----------------------:|
| female |                     23 |                    28 |                    59 |
| male   |                     35 |                    27 |                    56 |

Below is a visualization of the age distributions for men and women in
each education category.
